services:
  dnd-bot:
    platform: linux/arm64
    build: .
    container_name: dnd-bot-prod
    restart: unless-stopped
    # ðŸ”§ STRATEGIA RAM DISK:
    # - /dev/shm Ã¨ usato per l'accumulatore del mixer streaming (I/O zero-latency)
    # - Se RAM scende sotto 10%, fallback automatico su disco (temp_mix/)
    # - 6GB = ~25% della RAM totale (23GB), sufficiente per sessioni multi-ora
    shm_size: '6gb'
    depends_on:
      - redis
      - ollama
    env_file:
      - .env
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434/v1
      - TZ=Europe/Rome
      # ðŸ”§ TUNING GC: 6GB Heap (bilanciato con Ollama e sistema)
      # Rimosso --optimize_for_size e --gc_interval=100 da qui perchÃ© Node 22 li blocca in NODE_OPTIONS
      - NODE_OPTIONS=--max-old-space-size=6144
      - UV_THREADPOOL_SIZE=32

    # ðŸ”§ FLAG GC ESPLICITI: Passati direttamente al comando di avvio
    command: node --optimize_for_size --gc_interval=100 dist/index.js

    deploy:
      resources:
        limits:
          # NON limitiamo CPU: Whisper usa nice + -t 3, self-limiting
          # ðŸ”§ 20GB lascia 3GB al sistema per stabilitÃ 
          memory: 20G
        reservations:
          memory: 2G

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

    volumes:
      - ./recordings:/app/recordings
      - ./data:/app/data
      - ./batch_processing:/app/batch_processing

  redis:
    image: redis:alpine
    container_name: redis-server-prod
    restart: always

    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

    volumes:
      - redis_data:/data
    # ðŸ”§ FIX BULLMQ: Policy cambiata da allkeys-lru a noeviction per evitare perdita job
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy noeviction

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    restart: always

    environment:
      # ðŸ”§ CONFIGURAZIONE CRITICA
      - OLLAMA_NUM_PARALLEL=1           # Una inferenza alla volta
      - OLLAMA_MAX_LOADED_MODELS=2      # ðŸ†• Llama3.2 + nomic-embed-text
      - OLLAMA_FLASH_ATTENTION=1        # Ottimizzazione memoria
      - OLLAMA_NUM_THREADS=4            # Usa tutti i core quando attivo
      - OLLAMA_MAX_QUEUE=10             # Aumentato per embedding batch

    deploy:
      resources:
        limits:
          # NON limitiamo CPU: lasciamo che nice di Whisper gestisca la prioritÃ 
          memory: 14G  # ðŸ”§ Spazio per 2 modelli + overhead
        reservations:
          memory: 8G

    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434"

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  ollama-puller:
    image: ollama/ollama:latest
    container_name: ollama-model-puller
    restart: "no"
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=ollama:11434
    entrypoint: /bin/sh
    command: -c "echo 'Waiting for Ollama server...';
      until ollama list > /dev/null 2>&1; do sleep 2; done;
      echo 'Pulling models...';
      ollama pull nomic-embed-text;
      ollama pull llama3.2:latest;
      echo 'Done!'"

volumes:
  ollama_data:
  redis_data:
